{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings(action = 'ignore')\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "\n",
    "import re\n",
    "from eunjeon import Mecab\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding, Dense, LSTM, Bidirectional, SpatialDropout1D, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras import backend as K\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 같은 결과 생성을 위한 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15828500\n"
     ]
    }
   ],
   "source": [
    "sd = random.randint(0,99999999)\n",
    "print(sd)\n",
    "\n",
    "np.random.seed(sd)\n",
    "random.seed(sd)\n",
    "os.environ['PYTHONHASHEED']=str(sd)\n",
    "\n",
    "config = tf.ConfigProto(intra_op_parallelism_threads=1,inter_op_parallelism_threads=1)\n",
    "tf.set_random_seed(sd)\n",
    "\n",
    "sess = tf.Session(graph=tf.get_default_graph(), config=config)\n",
    "K.set_session(sess)\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(295945, 4)\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv(\"C:/Users/ASUS/Documents/data_competition/KB_smishing_classification/data/train.csv\", encoding = 'ANSI')\n",
    "print(train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 전처리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning(소문자, 특수문자 제거, xxx제거, Bi-gram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocessing(text_list):\n",
    "    \n",
    "    stopwords = ['을', '를', '이', '가', '은', '는', 'null']\n",
    "    tokenizer = Mecab()\n",
    "    bigram_list = []\n",
    "    \n",
    "    for text in text_list:\n",
    "        txt = re.sub('[^가-힣a-z]', ' ', text.lower())\n",
    "        txt = re.sub('x{1,}', ' ', txt)\n",
    "        token = tokenizer.morphs(txt)\n",
    "        token = [t for t in token if t not in stopwords or type(t) != float]\n",
    "        bigram = [token[i] + '.' + token[i+1] for i in range(len(token) - 1)]\n",
    "        bigram_list.append(' '.join(bigram))\n",
    "        \n",
    "    return bigram_list\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['bigram'] = text_preprocessing(train.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 277242, 1: 18703})"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "Counter(train['smishing']) #target unbalancing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_data_sampling(train, seed=1234, a=3, b=3):\n",
    "    \n",
    "    train_nsm_idx=list(train[train['smishing']==0].index)\n",
    "    train_sm_idx=list(train[train['smishing']==1].index)\n",
    "    random.seed(seed)\n",
    "    train_nsm_idx = random.sample(train_nsm_idx, k=18703*a)\n",
    "    random.seed(seed)\n",
    "    train_sm_idx = random.choices(train_sm_idx, k=18703*b)\n",
    "    train_idx = train_nsm_idx + train_sm_idx\n",
    "    print(train_idx[:5])\n",
    "    random.shuffle(train_idx)\n",
    "    print(train_idx[:5])\n",
    "    return train_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[75413, 22902, 71920, 47563, 79410]\n",
      "[210196, 98059, 36717, 271066, 188673]\n",
      "(93515, 5)\n"
     ]
    }
   ],
   "source": [
    "trn_idx = train_data_sampling(train, seed=sd, a=3, b=2)\n",
    "df_train = train.iloc[trn_idx].reset_index(drop=True)\n",
    "print(df_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pre-processing(bi-gram -> sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_tokenizer(tokenizer, mname):\n",
    "    with open('C:/Users/ASUS/Documents/PreModel/Tokenizer/' + mname + '.pickle', 'wb') as f:\n",
    "        pickle.dump(tokenizer, f, protocol = pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text2sequence(train_text, max_len=1000):\n",
    "    \n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(train_text)\n",
    "    save_tokenizer(tokenizer, 'tokenizer')\n",
    "    train_X_seq = tokenizer.texts_to_sequences(train_text)\n",
    "    vocab_size = len(tokenizer.word_index) + 1\n",
    "    print('vocab_size : ', vocab_size)\n",
    "    X_train = pad_sequences(train_X_seq, maxlen = max_len)\n",
    "    return X_train, vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_size :  22261\n",
      "(93515, 660) (93515,)\n"
     ]
    }
   ],
   "source": [
    "train_y = df_train['smishing']\n",
    "train_X, vocab_size = text2sequence(df_train['bigram'], max_len = 660)\n",
    "print(train_X.shape, train_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    295945.000000\n",
       "mean         68.920658\n",
       "std          89.861211\n",
       "min           0.000000\n",
       "25%          15.000000\n",
       "50%          34.000000\n",
       "75%          81.000000\n",
       "max         664.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series([len(x.split()) for x in train['bigram']]).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_ = 0\n",
    "def auc_score(y_true, y_pred):\n",
    "    global auc_\n",
    "    try:\n",
    "        auc_ = roc_auc_score( y_true, y_pred, average='macro', sample_weight = None).astype('float32')\n",
    "    except ValueError:\n",
    "        pass\n",
    "    return auc_\n",
    "\n",
    "def auc(y_true, y_pred):\n",
    "    score = tf.py_func( lambda y_true, y_pred : auc_score(y_true, y_pred) , [y_true, y_pred], 'float32', stateful = False, name = 'sklearnAUC' )\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BiLSTM(vocab_size, max_len=1000):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, 128, input_length = max_len))\n",
    "    model.add(SpatialDropout1D(0.3))\n",
    "    model.add(Bidirectional(LSTM(64)))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(64, activation='tanh', kernel_regularizer = regularizers.l2(0.001)))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=[auc])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_save(model, mname):\n",
    "    model_json = model.to_json()\n",
    "    with open('C:/Users/ASUS/Documents/PreModel/Tokenizer/'+mname+'.json', 'w') as json_file : \n",
    "        json_file.write(model_json)\n",
    "    model.save_weights('C:/Users/ASUS/Documents/PreModel/Tokenizer/'+mname+'.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START TIME:  2020-02-24T15:04:30.734996\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 660, 128)          2849408   \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d (SpatialDr (None, 660, 128)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 128)               98816     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 2,956,545\n",
      "Trainable params: 2,956,545\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 65460 samples, validate on 28055 samples\n",
      "Epoch 1/50\n",
      "65460/65460 [==============================] - 1279s 20ms/sample - loss: 0.0525 - auc: 0.9970 - val_loss: 0.0068 - val_auc: 1.0000\n",
      "Epoch 2/50\n",
      "65460/65460 [==============================] - 1275s 19ms/sample - loss: 0.0067 - auc: 0.9999 - val_loss: 0.0234 - val_auc: 1.0000\n",
      "Epoch 3/50\n",
      "65460/65460 [==============================] - 1282s 20ms/sample - loss: 0.0032 - auc: 1.0000 - val_loss: 0.0018 - val_auc: 1.0000\n",
      "Epoch 4/50\n",
      "65460/65460 [==============================] - 1276s 19ms/sample - loss: 0.0016 - auc: 1.0000 - val_loss: 0.0012 - val_auc: 1.0000\n",
      "Epoch 5/50\n",
      "65460/65460 [==============================] - 1272s 19ms/sample - loss: 0.0013 - auc: 1.0000 - val_loss: 0.0013 - val_auc: 1.0000\n",
      "Epoch 6/50\n",
      "65460/65460 [==============================] - 1427s 22ms/sample - loss: 0.0031 - auc: 1.0000 - val_loss: 0.0084 - val_auc: 1.0000\n",
      "Epoch 7/50\n",
      "65460/65460 [==============================] - 1278s 20ms/sample - loss: 0.0039 - auc: 1.0000 - val_loss: 0.0050 - val_auc: 1.0000\n",
      "END TIME:  2020-02-24T17:36:04.053554\n"
     ]
    }
   ],
   "source": [
    "print('START TIME: ', datetime.now().isoformat())\n",
    "model = BiLSTM(vocab_size, max_len=660)\n",
    "early_stopping = EarlyStopping(patience=3, min_delta=0.00001)\n",
    "history = model.fit(train_X, train_y, epochs=50, batch_size=128, validation_split=0.3, callbacks=[early_stopping])\n",
    "\n",
    "model_save(model, 'model') # save trained model\n",
    "print('END TIME: ', datetime.now().isoformat())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text2sequence_test(tokenizer, test_text, max_len=1000):\n",
    "    test_seq = tokenizer.texts_to_sequences(test_text)\n",
    "    X_test = pad_sequences(test_seq, maxlen=max_len)\n",
    "    return X_test\n",
    "\n",
    "def get_prediction(test_file_path):\n",
    "    '''\n",
    "    Args: String\n",
    "    Return: Pandas DataFrame    \n",
    "    '''\n",
    "    \n",
    "    '''1. load test dataset'''\n",
    "    test = pd.read_csv(test_file_path)\n",
    "    \n",
    "    '''2. load model and tokenizer'''\n",
    "    with open('1_Model/tokenizer.pickle', 'rb') as f:\n",
    "        tokenizer_test = pickle.load(f)\n",
    "    with open('1_Model/model.json', 'r') as ff:\n",
    "        json_model = ff.read()\n",
    "    model_test = model_from_json(json_model)\n",
    "    model_test.load_weights('1_Model/model.h5')\n",
    "    \n",
    "    '''3. test data preprocessing'''\n",
    "    test['bigram'] = text_preprocessing(test.text)\n",
    "    test_X = text2sequence_test(tokenizer_test, test['bigram'], max_len=660)\n",
    "    \n",
    "    model_test.compile(optimizer='adam', loss='binary_crossentropy', metrics=[auc])\n",
    "\n",
    "    '''4. prediction'''\n",
    "    y_pred = model_test.predict(test_X, batch_size=128)\n",
    "    \n",
    "    '''5. make submission'''\n",
    "    test['smishing'] = y_pred\n",
    "    submission = test[['id','smishing']]\n",
    "    #submission.to_csv('submission.csv',index=False)\n",
    "\n",
    "    return submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
